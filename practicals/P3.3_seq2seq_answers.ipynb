{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/practicals/P3.3_seq2seq_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3.3 - Sequence to Sequence: Text Translation\n",
    "\n",
    "In this practical we will develop a model for translation of sentences from German to English using the sequence to sequence architecture. \n",
    "\n",
    "### Learning outcomes\n",
    "- Understand the basic concepts of a sequence to sequence (seq2seq) model\n",
    "- How to preprocess textual data.\n",
    "- How to train an seq2seq model for parametrisation of the joint probability distribution $P(y_0, ..., y_k | x_0, ..., x_n)$ over the words $Y$ in the target language, conditioned on the words $X$ of the source sentence.\n",
    "- How to develop a model for translation of sentences from $P(y_0, ..., y_k | x_0, ..., x_n)$.\n",
    "\n",
    "**References**\n",
    "* [1] *Ilya Sutskever, Oriol Vinyals, Quoc V. Le, \"Sequence to Sequence Learning with Neural Networks\"*, NIPS, 2014. https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We train a translation model on the multi30K dataset. The dataset was specifically designed for machine translation and evaluation tasks and contains short translations from/to English to/from German.\n",
    "\n",
    "We will be downloading the dataset via the `torchnlp` library (command to install via pip is next to import). The dataset is then processed in a similar way as in P3.1_rnn_classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-nlp\n",
      "  Using cached pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
      "Requirement already satisfied: numpy in /data/storage8/jwillems/anaconda3/envs/torch/lib/python3.9/site-packages (from pytorch-nlp) (1.20.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 2.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, pytorch-nlp\n",
      "Successfully installed pytorch-nlp-0.5.0 tqdm-4.60.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 29000\n",
      "Number of test sentences: 1000\n",
      "\n",
      "\n",
      "DE: Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "EN: A man in an orange hat starring at something.\n",
      "\n",
      "DE: Ein Boston Terrier läuft über saftig-grünes Gras vor einem weißen Zaun.\n",
      "EN: A Boston Terrier is running on lush green grass in front of a white fence.\n",
      "\n",
      "DE: Ein Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt.\n",
      "EN: A girl in karate uniform breaking a stick with a front kick.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchnlp.datasets import multi30k_dataset #pip install pytorch-nlp\n",
    "\n",
    "train_data, test_data = multi30k_dataset(train=True, test=True)\n",
    "\n",
    "\n",
    "print(f\"Number of training sentences: {len(train_data)}\")\n",
    "print(f\"Number of test sentences: {len(test_data)}\\n\\n\")\n",
    "\n",
    "\n",
    "test_iterator = iter(test_data)\n",
    "for _ in range(3):\n",
    "    batch = next(test_iterator)\n",
    "    print(\"DE: \" + batch['de'])\n",
    "    print(\"EN: \" + batch['en'] + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing textual input data\n",
    "\n",
    "### Create vocabulary\n",
    "As we have seen in practical P1.2 and P3.2, word embeddings are useful for encoding words into vectors of real numbers. The first step is to build a custom vocabulary from the raw training dataset. To this end, we tokenize each sentence and thereafter count the number of occurances of each token (=word or punctuation mark) in each of the articles using `counter`. Finally, we create the vocabulary by using the frequencies of each token in the counter. \n",
    "\n",
    "Note that each datapoint consists of a German and English sentence, thus we create seperate tokenizers and vocabulary for both languages. Futhermore, we add special tokens to both vocabulary: $<unk>$ for unknown tokens, $<pad>$ for padding, $<start>$ and $<end>$ as the first and last tokens of each sentence, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download de\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/storage8/jwillems/anaconda3/envs/torch/lib/python3.9/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n",
      "/data/storage8/jwillems/anaconda3/envs/torch/lib/python3.9/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715e4a9720a04acbb3ac950d5ad4645e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 8014\n",
      "Unique tokens in target (en) vocabulary: 6191\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "de_counter, en_counter = Counter(), Counter()\n",
    "\n",
    "de_tokenizer = de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "for batch in tqdm(train_data):\n",
    "    \n",
    "    en, de = batch.values()\n",
    "    \n",
    "    de_counter.update(de_tokenizer(de))\n",
    "    en_counter.update(en_tokenizer(en))\n",
    "    \n",
    "    \n",
    "de_vocab = Vocab(de_counter, min_freq=2, specials=['<unk>', '<start>', '<stop>', '<pad>'])\n",
    "en_vocab = Vocab(en_counter, min_freq=2, specials=['<unk>', '<start>', '<stop>', '<pad>'])\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(de_vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(en_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipelines \n",
    "\n",
    "In the paper we are implementing, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\". We adopt this approach and reverse the German sentence after it has been transformed into a list of tokens.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Complete the pipeline functions that preprocess German and English sentences respectively. The German sentences should be reversed first. Then, for both German and English sentences your code should add start and stop tokens to each sentence at appropriate positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_pipeline(text):\n",
    "    \"\"\"\n",
    "    Reverses German sentence and tokizes from a string into a list of strings (tokens). Then converts each token\n",
    "    to corresponding indices. Furthermore, it adds a start token at the appropriate position.\n",
    "    \"\"\"\n",
    "    ### Your code here ###\n",
    "    \n",
    "    word_idcs = [de_vocab['<start>']]\n",
    "    \n",
    "    [word_idcs.append(de_vocab.stoi[token]) for token in de_tokenizer(text)[::-1]];\n",
    "    word_idcs.append(de_vocab['<stop>'])\n",
    "    \n",
    "    return word_idcs\n",
    "\n",
    "def en_pipeline(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English sentence from a string into a list of strings (tokens), then converts each token\n",
    "    to corresponding indices. Furthermore, it adds a start token at the appropriate position\n",
    "    \"\"\"\n",
    "    ### Your code here ###\n",
    "    \n",
    "    word_idcs = [en_vocab['<start>']]\n",
    "    [word_idcs.append(en_vocab.stoi[token]) for token in en_tokenizer(text)]\n",
    "    word_idcs.append(de_vocab['<stop>'])\n",
    "    \n",
    "    return word_idcs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipelines allow us to convert a string sentence into integers:\n",
    "\n",
    "    en_pipeline('Here is an example!')\n",
    "    >>> [1, 1034, 10, 28, 0, 1208, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Use the pipelines from the previous exercise to create a `collate_batch` method produces batches of source and target sentences. As you may have foreseen, the `collate_batch` will be used in the `DataLoader` which enables iterating over the dataset in batches. In each iteration, a batch of source sentences (German) and target sentences (English) should be returned. Encode the tokens of the sentences as indices by using the vocabulary. Finally, your code should pad all sequences to be able to create two tensors: one containing the input sentences, and another one for the target sentences. Pad the sequences with the appropriate special token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# check if gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Concatenate multiple datapoints to obtain a single batch of data\n",
    "    \"\"\"\n",
    "    ### Your code here ###\n",
    "    de_list, en_list = [], []\n",
    "    \n",
    "    # iterate over each sentence language pair in the batch\n",
    "    for item in batch:\n",
    "        \n",
    "        _en_sentence, _de_sentence = item.values()\n",
    "              \n",
    "        # process label using the label_pipeline and \n",
    "        # append this processed label to a list\n",
    "        de_list.append(torch.tensor(de_pipeline(_de_sentence), dtype=torch.int64))\n",
    "        \n",
    "        # process the input text using the text_pipeline\n",
    "        # and append this processed text to a list\n",
    "        en_list.append(torch.tensor(en_pipeline(_en_sentence), dtype=torch.int64))\n",
    "        \n",
    "    # pad sequences\n",
    "    de_padded = pad_sequence(sequences = de_list, \n",
    "                             batch_first = True, \n",
    "                             padding_value = de_vocab['<pad>'])\n",
    "    # pad sequences\n",
    "    en_padded = pad_sequence(sequences = en_list, \n",
    "                             batch_first = True, \n",
    "                             padding_value = en_vocab['<pad>'])\n",
    "    \n",
    "    # return source (DE) and target sequences (EN) after transferring them to GPU (if available)\n",
    "    return de_padded.to(device).T, en_padded.to(device).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Seq2Seq translation model\n",
    "\n",
    "In the implementation we define three objects: the encoder, the decoder and a full translation model that encapsulates the encoder and decoder. The given code also proposes the main hyperparameters that your implementation should use. Feel free to change the values of these parameters!\n",
    "\n",
    "The referenced paper uses a 4-layer LSTM, but in the interest of training time we can reduce this to 2-layers. The concept of multi-layer RNNs is easy to expand from 2 to 4 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder takes as input a (batch) German sentence. We already converted all sentences into a zero-padded 2D matrix (shape batch_size, max_seq_len)) containing the tokens that make up the sequences. \n",
    "\n",
    "**Exercise**:\n",
    "Complete the Encoder's class. In the `__init__(self)` you should declare the approriate layers. The encoder has to return a compact representation of the input sequence.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "As we have seen multiple times already, we convert each token into a vector with a word embedding layer. The embedding is then passed into the LSTM. PyTorch's LSTM initiates a hidden state of zeros (by default) and automatically updates the hidden state after seeing the word embedding of each token in the sequence. If trained properly, the rnn state of the last timestep (i.e. when all words in a sentence) contains all information that is required to translate into a different language. The rnn state of the final layer is therefore a dense representation of the input sentence that is generated by taking into account the long and short.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_vocab, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.source_vocab = source_vocab\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(source_vocab), emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Forward pass of encoder model. It aims at\n",
    "        transforming the input sentence to a dense vector \n",
    "        \n",
    "        Input:\n",
    "        src shape:  [max_seq_len_in_batch, batch_size]\n",
    "\n",
    "        Output:\n",
    "        hidden and cell dense vectors (hidden and cell)\n",
    "        which contains all sentence information, shape [n layers, batch size, hid dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]    \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "The next step is to implement the decoder. The Decoder class aims at performing a single step of decoding, i.e. it ouputs a single token per time-step. In the first decoding step ($t=1$), the decoder takes as input the dense representation first token $y_2 = f$(<<l>start>). With these inputs, it should update the cell and hidden state and thereafter predict the first real word $s_2$ (no start token) of the target sentence. In all later decoder steps, the first layer will receive a hidden and cell state from the previous time-step, $(h_{t-1}, c_{t-1})$, and feed it through the LSTM with the current embedded token, $y_t$ (i.e the embedding that of the token predicted at the end of the previous step), to produce a new hidden and cell state, $(h_t, c_t)$. \n",
    "\n",
    "You should then pass the hidden state of the RNN, $h_t$, through a linear layer, $g$, to make a prediction of what the next token in the target (output) sequence should be, i.e. $\\hat{y}_{t+1} = g(h_t)$. An example is provided in the diagram below.\n",
    "\n",
    "![alt text](lstm_decoder.png \"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "**Solution**\n",
    "    \n",
    "The arguments and initialization are similar to the Encoder class, except we now have an output_dim which is the size of the vocabulary for the output/target. There is also the addition of the Linear layer, used to make the predictions from the top layer hidden state.\n",
    "Within the forward method, we accept a batch of input tokens, previous hidden states and previous cell states. As we are only decoding one token at a time, the input tokens will always have a sequence length of 1. We unsqueeze the input tokens to add a sentence length dimension of 1. Then, similar to the encoder, we pass through an embedding layer and apply dropout. This batch of embedded tokens is then passed into the RNN with the previous hidden and cell states. This produces an output (hidden state from the top layer of the RNN), a new hidden state (one for each layer, stacked on top of each other) and a new cell state (also one per layer, stacked on top of each other). We then pass the output (after getting rid of the sentence length dimension) through the linear layer to receive our prediction. We then return the prediction, the new hidden state and the new cell state.\n",
    "\n",
    "**Note**: as we always have a sequence length of 1, we could use nn.LSTMCell, instead of nn.LSTM, as it is designed to handle a batch of inputs that aren't necessarily in a sequence. nn.LSTMCell is just a single cell and nn.LSTM is a wrapper around potentially multiple cells. Using the nn.LSTMCell in this case would mean we don't have to unsqueeze to add a fake sequence length dimension, but we would need one nn.LSTMCell per layer in the decoder and to ensure each nn.LSTMCell receives the correct initial hidden state from the encoder. All of this makes the code less concise - hence the decision to stick with the regular nn.LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_vocab = target_vocab\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(target_vocab), emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, len(target_vocab))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder model. It aims at transforming\n",
    "        the dense representation of the encoder into a sentence in\n",
    "        the target language\n",
    "        \n",
    "        Input:\n",
    "        hidden shape: [n layers, max_seq_len, hid dim]\n",
    "        cell shape: [n layers, batch size, hid dim]\n",
    "        input shape: [batch size]\n",
    "        \n",
    "        Output:\n",
    "        prediction shape: [batch size, num_words_target_vocabulary]\n",
    "        hidden shape: [n layers, batch size, hid dim]\n",
    "        cell shape: [n layers, batch size, hid dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The seq2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "The Seq2Seq model takes in an Encoder, Decoder, and a device (used to place tensors on the GPU, if it exists).\n",
    "For this implementation, we you have to ensure that the number of layers and the hidden (and cell) dimensions are equal in the Encoder and Decoder. \n",
    "\n",
    "Start with declaring the optimizer and loss function of the model. The loss function should not penalize if the ground truth token is the <<l>pad> token. Use the `ignore_index` input argument of the loss function to realize this behavior.\n",
    "\n",
    "\n",
    "The forward method takes the source sentence, target sentence and a teacher-forcing ratio. The teacher forcing ratio is used when training our model. When decoding, at each time-step the decoder will predict what the next token in the target sequence will be from the previous tokens decoded, $\\hat{y}_{t+1}=f(s_t)$. With probability equal to the teaching forcing ratio (`teacher_forcing_ratio`) we will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step. However, with probability 1 - `teacher_forcing_ratio`, your model should use the token that the LSTM predicted at the end of the previous step, even if it doesn't match the actual next token in the sequence. The `random.random()` will be useful here, the module has already been imported.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**    \n",
    "The first thing we do in the forward method is to create an outputs tensor that will store all of our predictions, $\\hat{Y}$.\n",
    "\n",
    "We then feed the input/source sentence, src, into the encoder and receive out final hidden and cell states.\n",
    "The first input to the decoder is the start of sequence ($<start>$) token. As our `trg` tensor already has the $<start>$ token appended (see pipelimes) we get our $y_1$ by slicing into it. We know how long our target sentences should be (max_len), so we loop that many times. The last token input into the decoder is the one before the <<l>stop> token - the $<stop>$ token is never input into the decoder.\n",
    "During each iteration of the loop, we:\n",
    "    \n",
    "- pass the input, previous hidden and previous cell states ($y_t, s_{t-1}, c_{t-1}$) into the decoder\n",
    "     \n",
    "- receive a prediction, next hidden state and next cell state ($\\hat{y}_{t+1}, s_{t}, c_{t}$) from the decoder\n",
    "     \n",
    "- place our prediction, $\\hat{y}_{t+1}$ output in our tensor of predictions `outputs`\n",
    "     \n",
    "- decide if we are going to \"teacher force\" or not\n",
    "- if we do, the next input is the ground-truth next token in the sequence, `trg[t]`\n",
    "- if we don't, the next input is the predicted next token in the sequence, `top1`, which we get by performing an argmax over the output tensor\n",
    "    \n",
    "    \n",
    "Once we've made all of our predictions, we return our tensor full of predictions `outputs`\n",
    "    \n",
    "    \n",
    "Note: our decoder loop starts at 1, not 0. This means the 0th element of our outputs tensor remains all zeros. So our trg and outputs look something like:\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [<start>&, y_1, y_2, y_3, <stop>]\\\\\n",
    "\\text{outputs} = [0&, y_1, y_2, y_3, <stop>]\n",
    "\\end{align*}$$\n",
    "Later on when we calculate the loss, we cut off the first element of each tensor to get:\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [y_1, y_2, y_3, <stop>]\\\\\n",
    "\\text{outputs} = [y_1, y_2, y_3, <stop>]\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        ### Your code here ###\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "        \n",
    "        TRG_PAD_IDX = en_vocab.stoi['<pad>']\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        Forward pass of the seq2seq model. It encodes the source sentence into\n",
    "        a dense representation and thereafter transduces into the target\n",
    "        sentence.\n",
    "        \n",
    "        Inputs:\n",
    "        src: padded index representation of source sentences with shape [src len, batch size]\n",
    "        trg:  padded index representation of target sentences with shape [trg len, batch size]\n",
    "        teacher_forcing_ratio: probability to use teacher forcing, e.g. 0.5 we use ground-truth target sentence 50% of the time\n",
    "        \n",
    "        Outputs:\n",
    "        outputs: padded index representation of the predicted sentences with shape [trg_len, batch_size, trg_vocab_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = len(self.decoder.target_vocab)\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Write functions for training and evaluating your model. You should iterate over the dataset and update the weights of the networks with the computed loss value. Print the value of training and validation loss at the end of each epoch. \n",
    "\n",
    "Next, you will need to call your `seq2seq` model and train it using the functions that you implemented. Finally, make a plot of the training and validation accuracy.\n",
    "\n",
    "As the model needs extensive training, it could be useful to save the best model to your (local) drive. In this way, you can do the next exercise at another time. Use the following code inside your training loop:\n",
    "    \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(seq2seq.state_dict(), 'p3.3-model.pt')\n",
    "        \n",
    "Don't forget to declare `best_valid_loss` at the top of the cell, e.g. with \n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "Finally, the GPU memory will gradually increase which eventually triggers a memory error. Make sure to clear the GPU memory before running the forward pass using the `torch.cuda.empty_cache()` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "The training and evaluation functions of P3.1 and P3.2 can be used with some  modifications: via tensor slicing we neglect the first token of each sequence in the calculations of the loss and accuracy. Furthermore, we removed the accuracy metrics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "DROPOUT = 0.5\n",
    "N_LAYERS = 2 #paper uses 4\n",
    "\n",
    "EMB_DIM = 256  #dimension of the word embedding\n",
    "HIDDEN_DIM = 512 #dimension of the lstm's hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(dataset, clip):\n",
    "    \n",
    "    seq2seq.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "    \n",
    "    for i, (src, trg) in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        seq2seq.optimizer.zero_grad()\n",
    "        \n",
    "        output = seq2seq(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = seq2seq.criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), clip)\n",
    "        \n",
    "        seq2seq.optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(dataset):\n",
    "    \n",
    "    seq2seq.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (src, trg) in enumerate(dataloader):\n",
    "\n",
    "            output = seq2seq(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].reshape(-1, output_dim)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = seq2seq.criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8014, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(6191, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=6191, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate seq2seq translation model\n",
    "enc = Encoder(de_vocab, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(en_vocab, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "seq2seq = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "seq2seq.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9833578d6f41a98e84d5b366efc61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 39.02s | train loss    5.062 valid loss    5.026 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea85728a8ea4adda08c2ba4fc5d3be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 36.66s | train loss    4.518 valid loss    4.909 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec19d45671e4245aa82b79de34a456d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 36.71s | train loss    4.223 valid loss    4.627 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f8687c9618457a9766e12699fb0796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 38.51s | train loss    3.983 valid loss    4.561 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66c34c7b47744aaaa5dc5ec1b5ebff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 37.81s | train loss    3.826 valid loss    4.368 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd9358f16a74545b24fccb59a416d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 36.63s | train loss    3.685 valid loss    4.293 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d18b1f2cee2485baaaab5819b3f2cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 37.01s | train loss    3.525 valid loss    4.222 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d98ad1bfd140099113a02fbb68c4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 40.83s | train loss    3.364 valid loss    4.149 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c20b2724f9455eac8c1196a7a780ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 38.12s | train loss    3.264 valid loss    4.067 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53810183630940dcb6266c2b0d33d03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 36.84s | train loss    3.135 valid loss    4.011 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d3175bc7fe4519a0190d84f1470e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 39.04s | train loss    3.009 valid loss    3.931 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4b7fd7023f4273b43d8a2608d0ce84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 37.03s | train loss    2.918 valid loss    3.938 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf8f03f9034422a92f1016c327624a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 37.06s | train loss    2.812 valid loss    3.875 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafe4e2d21bd440093ba73c2581d3ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 38.97s | train loss    2.729 valid loss    3.843 \n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f3bd2c2b09445fbd15febfbf126178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 38.95s | train loss    2.662 valid loss    3.823 \n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "train_loss_arr = []; val_loss_arr = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_loss = train(train_data, CLIP)\n",
    "    val_loss = evaluate(test_data)\n",
    "    \n",
    "    train_loss_arr.append(train_loss); val_loss_arr.append(val_loss)\n",
    "    \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(seq2seq.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print('-' * 76)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'train loss {:8.3f} '\n",
    "          'valid loss {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time, \n",
    "                                           train_loss_arr[-1],\n",
    "                                           val_loss_arr[-1]))\n",
    "    print('-' * 76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/storage8/jwillems/anaconda3/envs/torch/lib/python3.9/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['CLIP', 'random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Make a plot with training/testing accuracy vs. epochs ###\n",
    "%pylab inline\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "\n",
    "\n",
    "fnt=16\n",
    "ax.plot(train_loss_arr, color='blue', label='Train')\n",
    "ax.plot(val_loss_arr, color='red', linestyle='--', label='Test')\n",
    "ax.legend(fontsize=fnt)\n",
    "ax.tick_params(axis='both', labelsize=fnt)\n",
    "\n",
    "ax.set_xlabel(\"Epoch\", fontsize=fnt)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=fnt);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The trained model parametrizes the joint probability distribution $P(Y|X)$ of an English target sentence $Y$ that is a correct translation of the German source sentence $X$. Formally, we seek the sentence $Y$ which maximizes $P(Y|X)$, i.e. \n",
    "\n",
    "$$\n",
    "Y = \\underset{Y^{'}}{\\operatorname{argmax}} p(Y^{′}|X). \\quad{(1)}\n",
    "$$\n",
    "\n",
    "**Exercise** \n",
    "\n",
    "During inference using the seq2seq model you can make certain assumptions that should affect your implementation choices. You can assume conditional indepedance of the targets $P(Y|X)=P(y_{0:k}|X)=P(y_0|X)P(y_1|X)...P(y_k|X)$. In this case you can implement a greedy decoder that computes the most likely output at each step without taking into acount the selected outputs at previous steps. Or you can implement an autoregressive decoder that computes the joint probability of the output given the input $P(Y|X)=P(y_{0:k}|X)=P(y_0|X)P(y_1|y_0, X)...P(y_k|y_{0:k-1},X)$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " src: Ein Mann mit einem orangefarbenen Hut , der etwas .\n",
      " trg: A man in an orange hat starring at something .\n",
      " pred: A man with a blue hat is welding .\n",
      "\n",
      " src: Ein Boston Terrier läuft über Gras vor einem weißen Zaun .\n",
      " trg: A Boston Terrier is running on lush green grass in front of a white fence .\n",
      " pred: A skier is running a a white dog in front of a building .\n",
      "\n",
      " src: Ein Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt .\n",
      " trg: A girl in karate uniform breaking a stick with a front kick .\n",
      " pred: A girl in a blue outfit is a a a a .\n",
      "\n",
      " src: Fünf Leute in Winterjacken und mit Helmen stehen im Schnee mit im Hintergrund .\n",
      " trg: Five people wearing winter jackets and helmets stand in the snow , with in the background .\n",
      " pred: Five people in life uniforms and red vests are in in the snow with trees in the background .\n",
      "\n",
      " src: Leute das Dach eines Hauses .\n",
      " trg: People are fixing the roof of a house .\n",
      " pred: Five people are working on a scaffold .\n",
      "\n",
      " src: Ein hell gekleideter Mann fotografiert eine Gruppe von Männern in dunklen Anzügen und mit Hüten , die um eine Frau in einem Kleid herum stehen .\n",
      " trg: A man in light colored clothing photographs a group of men wearing dark suits and hats standing around a woman dressed in a gown .\n",
      " pred: A man in a and a man in a middle of a woman in a a woman in a white coat and a . .\n",
      "\n",
      " src: Eine Gruppe von Menschen steht vor einem Iglu .\n",
      " trg: A group of people standing in front of an igloo .\n",
      " pred: A group of people standing outside a a . .\n",
      "\n",
      " src: Ein Junge in einem roten Trikot versucht , die Home Base zu erreichen , während der im blauen Trikot versucht , ihn zu fangen .\n",
      " trg: A boy in a red uniform is attempting to avoid getting out at home plate , while the catcher in the blue uniform is attempting to catch him .\n",
      " pred: A boy in a blue jersey is trying to get the ball in the in the in the in the . .\n",
      "\n",
      " src: Ein Typ arbeitet an einem Gebäude .\n",
      " trg: A guy works on a building .\n",
      " pred: A guy working on a rock .\n",
      "\n",
      " src: Ein Mann in einer Weste sitzt auf einem Stuhl und hält .\n",
      " trg: A man in a vest is sitting in a chair and holding magazines .\n",
      " pred: A man in a hat is sitting on a chair and .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def idx_to_sen(sentence_idcs, vocab):\n",
    "    \n",
    "    sentence_idcs = sentence_idcs[sentence_idcs > 3] #remove special tokens\n",
    "                \n",
    "    sentence_idcs = np.array(vocab.itos)[sentence_idcs]\n",
    "                \n",
    "    return ' '.join(sentence_idcs)\n",
    "    \n",
    "    \n",
    "def print_test_examples(src, trg, pred, N):\n",
    "    \n",
    "    for src_, trg_, pred_ in zip(src[:N], trg[:N], pred[:N]):\n",
    "        print(f' src: {src_}\\n trg: {trg_}\\n pred: {pred_}\\n')\n",
    "    \n",
    "    \n",
    "def greedy_decoder(dataset):\n",
    "    \n",
    "    seq2seq.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "    \n",
    "    predf = []; srcf = []; trgf = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (src, trg) in enumerate(dataloader):\n",
    "\n",
    "            output = seq2seq(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            top1 = output.argmax(2)\n",
    "            \n",
    "            \n",
    "            for p, s, t in zip(top1.T.cpu(), src.T.cpu(), trg.T.cpu()):\n",
    "                predf.append(idx_to_sen(p, en_vocab))\n",
    "                srcf.append(idx_to_sen(torch.flip(s, (0, )), de_vocab))\n",
    "                trgf.append(idx_to_sen(t, en_vocab)) \n",
    "        \n",
    "        return srcf, trgf, predf\n",
    "\n",
    "out = greedy_decoder(test_data)\n",
    "\n",
    "print_test_examples(*out, N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
