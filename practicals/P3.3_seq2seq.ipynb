{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/practicals/P3.3_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3.3 - Sequence to Sequence: Text Translation\n",
    "\n",
    "In this practical we will develop a model for translation of sentences from German to English using the sequence to sequence architecture. \n",
    "\n",
    "### Learning outcomes\n",
    "- Understand the basic concepts of a sequence to sequence (seq2seq) model\n",
    "- How to preprocess textual data.\n",
    "- How to train an seq2seq model for parametrisation of the joint probability distribution $P(y_0, ..., y_k | x_0, ..., x_n)$ over the words $Y$ in the target language, conditioned on the words $X$ of the source sentence.\n",
    "- How to develop a model for translation of sentences from $P(y_0, ..., y_k | x_0, ..., x_n)$.\n",
    "\n",
    "**References**\n",
    "* [1] *Ilya Sutskever, Oriol Vinyals, Quoc V. Le, \"Sequence to Sequence Learning with Neural Networks\"*, NIPS, 2014. https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We train a translation model on the IWSLT2016 dataset that can be accessed through the `torchtext` library. The dataset was specifically designed for machine translation and evaluation tasks and contains translations from/to English to/from Arabic, Czech, French, German. We restrict ourselves to German-English translation, i.e. we download only the DE, EN language pairs. \n",
    "\n",
    "Note that we are in a similar setting as in P3.1_rnn_classification since we are downloading the dataset from the torchtext library. This implies that we can preprocess the datasets in the same way as in the aforementioned practical session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Number of training sentences: 10000\n",
      "Number of validation sentences: 993\n",
      "Number of test sentences: 1305\n",
      "\n",
      "\n",
      "DE: When I was in my 20s, I saw my very first psychotherapy client.\n",
      "\n",
      "EN: Als ich in meinen 20ern war, hatte ich meine erste Psychotherapie-Patientin.\n",
      "\n",
      "\n",
      "DE: I was a Ph.D. student in clinical psychology at Berkeley.\n",
      "\n",
      "EN: Ich war Doktorandin und studierte Klinische Psychologie in Berkeley.\n",
      "\n",
      "\n",
      "DE: She was a 26-year-old woman named Alex.\n",
      "\n",
      "EN: Sie war eine 26-j√§hrige Frau namens Alex.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data     #pip install torchtext\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import Subset, Dataset, IterableDataset\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "# downloading dataset may take a while...\n",
    "train_iter, val_iter, test_iter = datasets.IWSLT2016(split=('train', 'valid', 'test'), language_pair=('de', 'en'))\n",
    "\n",
    "\n",
    "# we take a subset of the data to prevent memory issues in Colab \n",
    "N=10000 #increasing this may cause Colab crashes..\n",
    "train_iter = Subset(train_iter, torch.arange(N)).dataset\n",
    "train_iter.num_lines = N\n",
    "\n",
    "print(f\"Number of training sentences: {len(train_iter)}\")\n",
    "print(f\"Number of validation sentences: {len(val_iter)}\")\n",
    "print(f\"Number of test sentences: {len(test_iter)}\\n\\n\")\n",
    "\n",
    "for _ in range(3):\n",
    "    en, de = next(test_iter)\n",
    "    print(\"DE: \" + de)\n",
    "    print(\"EN: \" + en + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing textual input data\n",
    "\n",
    "### Create vocabulary\n",
    "As we have seen in practical P1.2 and P3.2, word embeddings are useful for encoding words into vectors of real numbers. The first step is to build a custom vocabulary from the raw training dataset. To this end, we tokenize each sentence and thereafter count the number of occurances of each token (=word or punctuation mark) in each of the articles using `counter`. Finally, we create the vocabulary by using the frequencies of each token in the counter. \n",
    "\n",
    "Note that each datapoint consists of a German and English sentence, thus we create seperate tokenizers and vocabulary for both languages. Futhermore, we add special tokens to both vocabulary: $<unk>$ for unknown tokens, $<start>$ and $<end>$ as the first and last tokens of each sentence, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install prerequisite modules\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/storage8/jwillems/anaconda3/envs/torch/lib/python3.7/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n",
      "/data/storage8/jwillems/anaconda3/envs/torch/lib/python3.7/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cabae255f24442a6569fb6ec1e34b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique tokens in source (de) vocabulary: 20053\n",
      "Unique tokens in target (en) vocabulary: 13226\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "de_counter, en_counter = Counter(), Counter()\n",
    "\n",
    "for (de, en) in tqdm(train_iter):\n",
    "    de_counter.update(de_tokenizer(de))\n",
    "    en_counter.update(en_tokenizer(en))\n",
    "\n",
    "de_vocab = Vocab(de_counter, min_freq=1, specials=['<unk>', '<start>', '<stop>'])\n",
    "en_vocab = Vocab(en_counter, min_freq=1, specials=['<unk>', '<start>', '<stop>'])\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(de_vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(en_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipelines \n",
    "\n",
    "In the paper we are implementing, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\". We adopt this approach and reverse the German sentence after it has been transformed into a list of tokens.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Complete the pipeline functions that preprocess German and English sentences respectively. The German sentences should be reversed first. Then, for both German and English sentences your code should add start and stop tokens to each sentence at appropriate positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_pipeline(text):\n",
    "    \"\"\"\n",
    "    Reverses German sentence and tokizes from a string into a list of strings (tokens). Then converts each token\n",
    "    to corresponding indices. Furthermore, it adds start and stop tokens at the appropriate positions.\n",
    "    \"\"\"\n",
    "    ### Your code here ###\n",
    "    \n",
    "    return word_idcs\n",
    "\n",
    "def en_pipeline(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English sentence from a string into a list of strings (tokens), then converts each token\n",
    "    to corresponding indices. Furthermore, it adds start and stop tokens at the appropriate positions\n",
    "    \"\"\"\n",
    "    ### Your code here ###\n",
    "    \n",
    "    return word_idcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipelines allow us to convert a string sentence into integers:\n",
    "\n",
    "    en_pipeline('Here is an example!')\n",
    "    >>> [1, 316, 14, 53, 241, 283, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Use the pipelines from the previous exercise to create a `collate_batch` method produces batches of source and target sentences. As you may have foreseen, the `collate_batch` will be used in the `DataLoader` which enables iterating over the dataset in batches. In each iteration, a batch of source sentences (German) and target sentences (English) should be returned. Encode the tokens of the sentences as indices by using the vocabulary. Finally, your code should pad all sequences to be able to create two tensors: one containing the input sentences, and another one for the target sentences. Pad the sequences with the appropriate special token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# check if gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Concatenate multiple datapoints to obtain a single batch of data\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Your code here ###\n",
    "    \n",
    "    # return source (DE) and target sequences (EN) after transferring them to GPU (if available)\n",
    "    return de_padded.to(device), en_padded.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Seq2Seq translation model\n",
    "\n",
    "In the implementation we define three objects: the encoder, the decoder and a full translation model that encapsulates the encoder and decoder. The given code also proposes the main hyperparameters that your implementation should use. Feel free to change the values of these parameters!\n",
    "\n",
    "The referenced paper uses a 4-layer LSTM, but in the interest of training time we can reduce this to 2-layers. The concept of multi-layer RNNs is easy to expand from 2 to 4 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "N_LAYERS = 2 #paper uses 4\n",
    "\n",
    "EMB_DIM = 256  #dimension of the word embedding\n",
    "HIDDEN_DIM = 512 #dimension of the lstm's hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder takes as input a (batch) German sentence. We already converted all sentences into a zero-padded 2D matrix (shape batch_size, max_seq_len)) containing the tokens that make up the sequences. \n",
    "\n",
    "**Exercise**:\n",
    "Complete the Encoder's class. In the `__init__(self)` you should declare the approriate layers. The encoder has to return a compact representation of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, source_vocab=de_vocab, emb_dim=EMB_DIM, hid_dim=HIDDEN_DIM, dropout=DROPOUT, n_layers=N_LAYERS):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.source_vocab = source_vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        ### Your code here ###\n",
    "    \n",
    "        \n",
    "    def forward(self, padded_word_idcs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass of encoder model. It aims at\n",
    "        transforming the input sentence to a dense vector \n",
    "        \n",
    "        Input:\n",
    "        padded_word_idcs shape:  (batch_size, max_seq_len_in_batch)\n",
    "\n",
    "        Output:\n",
    "        a dense vector\n",
    "        which contains all sentence information\n",
    "        \"\"\"\n",
    "        \n",
    "        ### Your code here ###\n",
    "        \n",
    "        #hidden = [n layers, max_seq_len, hid dim]\n",
    "        #cell = [n layers, max_seq_len, hid dim]\n",
    "       \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "The next step is to implement the decoder. The Decoder class aims at performing a single step of decoding, i.e. it ouputs a single token per time-step. In the first decoding step ($t=1$), the decoder takes as input the dense representation first token $y_2 = f$(<<l>start>). With these inputs, it should update the cell and hidden state and thereafter predict the first real word $s_2$ (no start token) of the target sentence. In all later decoder steps, the first layer will receive a hidden and cell state from the previous time-step, $(h_{t-1}, c_{t-1})$, and feed it through the LSTM with the current embedded token, $y_t$ (i.e the embedding that of the token predicted at the end of the previous step), to produce a new hidden and cell state, $(h_t, c_t)$. \n",
    "\n",
    "You should then pass the hidden state of the RNN, $h_t$, through a linear layer, $g$, to make a prediction of what the next token in the target (output) sequence should be, i.e. $\\hat{y}_{t+1} = g(h_t)$. An example is provided in the diagram below.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/vlamen/tue-deeplearning/main/img/lstm_decoder.png \"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab, emb_dim=EMB_DIM, hid_dim=HIDDEN_DIM, dropout=DROPOUT, n_layers=N_LAYERS,):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_vocab = target_vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        ### Your code here ###\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, cell, padded_word_idcs):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder model. It aims at transforming\n",
    "        the dense representation of the encoder into a sentence in\n",
    "        the target language\n",
    "        \n",
    "        Input:\n",
    "        hidden shape: [n layers, max_seq_len, hid dim]\n",
    "        cell shape: [n layers * n directions, batch size, hid dim]\n",
    "        padded_word_idcs shape: [batch size]\n",
    "        \n",
    "        Output:\n",
    "        prediction shape: [batch size, num_words target_vocabulary]\n",
    "        hidden shape: [n layers, max_seq_len, hid dim]\n",
    "        cell shape: [n layers * n directions, batch size, hid dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        #### Your code here ###\n",
    "        \n",
    "        #prediction = [batch size, num_words target_vocabulary]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The seq2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "The Seq2Seq model takes in an Encoder, Decoder, and a device (used to place tensors on the GPU, if it exists).\n",
    "For this implementation, we you have to ensure that the number of layers and the hidden (and cell) dimensions are equal in the Encoder and Decoder. \n",
    "\n",
    "Start with declaring the optimizer and loss function of the model. The loss function should not penalize if the ground truth token is the <<l>stop> token. Use the `ignore_index` input argument of the loss function to realize this behavior.\n",
    "\n",
    "\n",
    "The forward method takes the source sentence, target sentence and a teacher-forcing ratio. The teacher forcing ratio is used when training our model. When decoding, at each time-step the decoder will predict what the next token in the target sequence will be from the previous tokens decoded, $\\hat{y}_{t+1}=f(s_t)$. With probability equal to the teaching forcing ratio (`teacher_forcing_ratio`) we will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step. However, with probability 1 - `teacher_forcing_ratio`, your model should use the token that the LSTM predicted at the end of the previous step, even if it doesn't match the actual next token in the sequence. The `random.random()` will be useful here, the module has already been imported.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "        ### Your code here ###\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "        stop_token_idx = self.decoder.target_vocab['<stop>']\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index = stop_token_idx)\n",
    "        \n",
    "    def forward(self, padded_src_sen, padded_trg_sen, teacher_forcing_ratio = 0.75):\n",
    "        \"\"\"\n",
    "        Forward pass of the seq2seq model. It encodes the source sentence into\n",
    "        a dense representation and thereafter transduces into the target\n",
    "        sentence.\n",
    "        \n",
    "        Inputs:\n",
    "        padded_src_sen: padded index representation of source sentences with shape [batch size, src len]\n",
    "        padded_trg_sen:  padded index representation of target sentences with shape [batch size, trg len]\n",
    "        teacher_forcing_ratio: probability to use teacher forcing, e.g. 0.75 we use ground-truth target sentence 75% of the time\n",
    "        \n",
    "        Outputs:\n",
    "        outputs: padded index representation of the predicted sentences with shape [batch_size, trg_len, trg_vocab_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = padded_src_sen.shape[0]\n",
    "        trg_len = padded_trg_sen.shape[1]\n",
    "        trg_vocab_size = len(self.decoder.target_vocab)\n",
    "        \n",
    "        ### Your code here ###\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Write functions for training and evaluating your model. You should iterate over the dataset and update the weights of the networks with the computed loss value. Use accuracy as a metric and print the evaluation accuracy at the end of each epoch. \n",
    "\n",
    "Next, you will need to call your `seq2seq` model and train it using the functions that you implemented. Finally, make a plot of the training and validation accuracy.\n",
    "\n",
    "As the model needs extensive training, it could be useful to save the best model to your drive. In this way, you can do the next exercise at another time. Use the following code inside your training loop:\n",
    "    \n",
    "\n",
    "    if val_acc[-1] > best_valid_acc:\n",
    "        best_valid_acc = val_acc[-1]\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "Don't forget to declare `best_valid_acc` at the top of the cell, e.g. with \n",
    "\n",
    "    best_valid_acc = float(0)\n",
    "    \n",
    "Finally, the GPU memory will gradually increase which eventually triggers a memory error. Make sure to clear the GPU memory before running the forward pass using the `torch.cuda.empty_cache()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataset):\n",
    "    \n",
    "    ### Your code here ###        \n",
    "    \n",
    "    return total_acc/total_count\n",
    "\n",
    "\n",
    "def evaluate(dataset):\n",
    "    \n",
    "    ### Your code here ###\n",
    "    \n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# empty the GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "best_valid_acc = float(0)\n",
    "\n",
    "# initiate seq2seq translation model\n",
    "enc = Encoder(de_vocab, EMB_DIM, HIDDEN_DIM, DROPOUT, N_LAYERS)\n",
    "dec = Decoder(en_vocab, EMB_DIM, HIDDEN_DIM, DROPOUT, N_LAYERS)\n",
    "\n",
    "seq2seq = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_acc, val_acc = [], []\n",
    "# training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_iter, val_iter = datasets.IWSLT2016(split=('train', 'valid'), language_pair=('de', 'en'))\n",
    "    train_iter = Subset(train_iter, torch.arange(N)).dataset\n",
    "    train_iter.num_lines = N\n",
    "    \n",
    "    train_acc.append(train(train_iter))\n",
    "\n",
    "    val_acc.append(evaluate(val_iter))\n",
    "    \n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           val_acc[-1]))\n",
    "    print('-' * 59)\n",
    "    \n",
    "    if val_acc[-1] > best_valid_acc:\n",
    "        best_valid_acc = val_acc[-1]\n",
    "        torch.save(seq2seq.state_dict(), 'tut1-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a plot with training/testing accuracy vs. epochs ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The trained model parametrizes the joint probability distribution $P(Y|X)$ of an English target sentence $Y$ that is a correct translation of the German source sentence $X$. Formally, we seek the sentence $Y$ which maximizes $P(Y|X)$, i.e. \n",
    "\n",
    "$$\n",
    "Y = \\underset{Y^{'}}{\\operatorname{argmax}} p(Y^{‚Ä≤}|X). \\quad{(1)}\n",
    "$$\n",
    "\n",
    "**Exercise** \n",
    "\n",
    "During inference using the seq2seq model you can make certain assumptions that should affect your implementation choices. You can assume conditional indepedence of the targets $P(Y|X)=P(y_{0:k}|X)=P(y_0|X)P(y_1|X)...P(y_k|X)$. In this case you can implement a greedy decoder that computes the most likely output at each step without taking into acount the selected outputs at previous steps. Or you can implement an autoregressive decoder that computes the joint probability of the output given the input $P(Y|X)=P(y_{0:k}|X)=P(y_0|X)P(y_1|y_0, X)...P(y_k|y_{0:k-1},X)$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}